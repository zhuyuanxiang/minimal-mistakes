---
title: "《神经网络与机器学习》的读书笔记 -- 第 0 章"
excerpt: "原名应该是" 神经网络与学习机器 ", 讲的是神经网络的学习机制。"
# classes: wide
categories:
- Algorithm
tags:
- Neural Network
- Learning Machine
- Machine Learning
- Deep Learning
create_at: 2019-02-27
last_modified_at: 2019-03-09
toc: true
toc_label: "文章提纲"
toc_icon: "book-reader"
toc_sticky: true
---

## 全书总评
* 书本印刷质量: 4 星。印刷清楚, 排版合适, 错误很少。只是 "机械工业出版社" 的教材好像纸张的质量都不是很好。
* 著作编写质量: 4 星。个人觉得是神经网络方面的经典教材, 比有些神经网络教材的数学推导方面要易于理解。
* 笔记目的: 记录重点, 方便回忆。

## C0. 导言
### 0.1. 神经网络
* 神经网络: 简单处理单元构成的大规模并行分布式处理器, 天然地具有存储经验知识和使之可用的特性。
  * 神经网络是通过学习过程从外界环境中获取知识; 
  * 互连神经元的连接强度, 即突触权值, 用于存储获取的知识。
* 学习算法: 用于完成学习过程的程序, 以有序的方式改变网络的突触权值以获得想要的设计目标。
* 神经网络的优点: 
  * 大规模并行分布式结构; 
  * 学习能力以及由此得到的泛化能力。
    * 泛化: 神经网络对未在训练 (学习) 过程中遇到的数据可以得到合理的输出。
* 神经网络的性质: 
  * 非线性: 分布于整个网络中。
  * 输入输出映射: 亦称有老师学习或监督学习。
    * 输入信号: 训练样本或任务样本; 
    * 输出信号: 期望响应或目标响应; 
    * 目标: 最小化期望响应和实际响应之间的差别。
  * 自适应性: 调整自身突触权值以适应外界环境变化
  * 证据响应: 在模式分类问题中, 可以提供关于决策的置信度信息。
  * 上下文信息: 特定结构和激发状态代表知识
  * 容错性: 并行分布式结构具有天生的容错性, 或者有鲁棒计算的能力, 在不得的运行条件下性能是逐渐下降的
  * VLSI 实现: 适合使用超大规模集成电路进行实现
  * 分析和设计的一致性: 作为信息处理器具有通用性。
  * 神经生物类比: 

### 0.3. 神经元模型
* 神经元模型的三处基本元素: 
  * 突触或连接链集: 输入信号乘以突触权值; 
  * 加法器: 对突触输出值求和; 
  * 激活函数: 限制神经元输出振幅。
    * 阈值函数
    * sigmoid 函数
* 神经元的统计模型: 

### 0.4. 用有向图描述神经网络
* 信号流图: 是一个由在特定的称为节点的点之间相连的有向连接 (分支) 组成的网络。
* 信号流动的三条基本规则: 
  * 信号仅仅沿着定义好的箭头方向在连接上流动; 
  * 节点信号等于经由连接进入的有关节点的所有信号的代数和; 
  * 节点信号沿每个外向连接向外传递, 此时传递的信号完全独立于外向连接的传递函数。
* 神经网络有向图的 4 个主要特征: 
  * 每个神经元可表示为一组线性的突触连接, 一个外部应用偏置, 以及可能的非线性激活连接; 
  * 神经元的突触给它们相应的输入信号加权; 
  * 输入信号的加权和构成该神经元的诱导局部域; 
  * 激活连接压制神经元的诱导局部域产生输出。
* 完全的有向图: 神经元间的信号流 + 每个神经元内部的信号流。
* 局部完全的有向图: 又称为神经网络结构图
  * 源节点向图提供输入信号; 
  * 每个神经元称为计算节点, 用单个节点表示; 
  * 联结图中源节点和计算节点之间的通信连接没有权值, 仅仅提供图中信号流的方向。
* 神经网络的三种图形表示方法: 
  * 方框图: 提供网络的功能描述; 
  * 结构图: 描述网络布局; 
  * 信号流图: 提供网络中完全的信号流描述。

### 0.5. 反馈 (信号处理系统的思路)
* 递归网络→单环反馈系统: 
* 算子: 
  * 闭环算子
  * 开环算子
  * 延迟算子
* 系统: 
  * 稳定性
  * 记忆性

### 0.6. 网络结构
* 单层前馈网络: 最简单的分层网络, 严格前馈的; 
* 多层前馈网络: 输入层 + 隐藏层 + 输出层
* 递归网络: 

### 0.7. 知识表示
* 知识: 就是人或机器储存起来以备使用的信息或模型, 用来对外部世界作出解释、预测和适当的反应。
  * 先验信息
  * 对世界的观测
* 知识表示的规则: 
  * 相似类别中的相似输入通常应产生网络中相似的表示, 因此可以归入同类
  * 网络对可分享为不同各类的输入向量给出差别很大的表示
  * 如果某个特征很重要, 那么网络需要使用大量神经元来表示这个向量
  * 如果存在先验信息和不变性, 应该将其附加在网络设计中, 这样就不必学习这些信息而简化网络设计
* 知识加入神经网络设计的技术: 
  * 通过使用接收域的局部连接, 限制网络结构
  * 通过使用权值共享, 限制突触权值的选择
* 不变性加入神经网络设计的技术: 
  * 结构不变性
  * 训练不变性
  * 不变特征空间

### 0.8. 学习过程
* 有教师学习: 也称为监督学习, 误差 - 修正学习, 闭环反馈系统
* 无教师学习: 
  * 强化学习: 学习是通过与环境的不断交互完成的, 使一个标量性能指标达到最小
  * 无监督学习: 任务独立度量来度量网络的表达质量, 让网络学习该试题而且将根据这个度量来最优化网络自由参数。

### 0.9. 学习任务
* 模式联想: 依靠联想学习的分布式记忆
  * 自联想: 检索 (回忆) 已经存储的模式, 无监督学习; 
  * 异联想: 任意的输入模式集合与输出模式集合配对, 有监督学习; 
  * 联想记忆模式操作的两个阶段: 
    * 存储阶段: 对网络进行训练
    * 回忆阶段: 恢复对应的存储模式
  * 存储能力: 正确回忆的模式数量
* 模式识别: 将接收到的模式或信号确定为一些指定类别的某个类; 
  * 作特征提取的无监督网络
  * 作分类的有监督网络: 遵循传统的统计特性模式识别方法
* 函数逼近: 
  * 系统辨识
  * 逆模型
* 系统控制: 对设备的输入输出行为进行转换
  * 间接学习
  * 直接学习
* 波束形成: 区分目标信号和背景噪声之间的空间性质

### 0.10. 结束语
学习的三个重要类别: 
1. 有监督学习: 通过最小化感兴趣的代价函数来实现特定的输入 - 输出映射, 需要提供目标或者期望的响应。需要依赖于有标记的训练样本, 每个样本由输入信号 (刺激) 和期望响应 (目标) 组成。
2. 无监督学习: 依赖于网络在自组织方式下学习所需要的对表示质量的 "任务独立度量"。利用无标记的训练样本进行聚类。
3. 半监督学习: 采用有标记和无标记的样本。
4. 强化学习: 学习系统通过持续地与其环境的交互来最小化一个标量性能指标, 从而实现输入 - 输出映射。

## C01.Rossenblatt 感知器
* 1.1. 神经网络的历史：线性可分模式
* <font color=red > 学习重点 </font>
  * 1.2. Rossenblatt 感知器的最基本形式：一个非线性神经元，即神经元的McCulloch-Pitts模型
  * 1.3. 感知器收敛定理
  * 1.6. 感知器的代价函数
* 只需要了解, 未来深入学习
  * 1.4. 高斯环境下感知器与贝叶斯分类器之间的关系
  * 1.5. 通过实验来了解感知器的分类能力
* 1.7. 总结与讨论

### 1.3. 感知器收敛定理
* 感知器的权值向量自适应算法：
  * 权值更新
  * 学习率参数
    * 较小的值可以提供稳定的权值估计
    * 较大的值可以快速地完成权值估计
  * 绝对误差修正过程
* 表1.1 感知器收敛算法概述

### 1.4. 高斯环境下感知器与贝叶斯分类器之间的关系
* 贝叶斯分类器是经典的统计学习下的模式分类器
* 研究高斯环境下感知器与贝叶斯分类器的关系有助于深入理解感知器的数学原理
* 相似和区别：
  * 都是线性分类器；
  * 两个高斯分布的模式是互相重叠的，不是线性可分的，而感知器只能处理线性可分的问题，因此对于高斯分布模型会持续振荡，无法收敛；
  * 贝叶斯分类器最小化分类误差的概率；
  * 感知器收敛算法是非参数的，而贝叶斯分类器是参数的；
  * 感知器收敛算法是自适应且实现简单的，而贝叶斯分类器是固定的。

### 1.6. 感知器的代价函数
* 感知器代价函数的广义形式
* 感知器收敛算法的批量处理