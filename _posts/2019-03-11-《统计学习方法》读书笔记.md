---
title: "《统计学习方法》的读书笔记"
excerpt: "“小蓝书” 的读书笔记，记录自己认为的重点内容。"
classes: wide
categories:
- Algorithm
tags:
- Machine Learning
- Statistics Learning
- Data Science
create_at: 2019-03-11
last_modified_at: 2019-03-20
toc: true
toc_label: "文章提纲"
toc_icon: "book-reader"
toc_sticky: true
---

# 全书总评

* 书本印刷质量: 4 星。印刷清楚, 排版合适, 错误很少。
* 著作编写质量: 4 星。自学机器学习的必备。
  * 优点：
    * 全书一直围绕着统计学习中的有监督学习描述，内容不深，基本算法都有所介绍；
    * 内容的组织是从抽象到具体的思维模式，比国外的教材易于理解；
    * 是自学统计学习和机器学习的推荐用书。
  * 缺点：
    * 基础部分讲解缺少理论，学完后无法理解，不利用学以致用。例如：感知器的损失函数，应该是统计学习的核心思想，那么损失函数在整个算法中的位置，以及如何选择损失函数都需要说明清楚，才能够指导后面各种其他机器学习方法的理解。
    * 使用的方法没有导入的原因和出处，学习过程中会产生比较大的跳跃感，延续性不足。例如：随机梯度下降法，只是说明用于神经网络的优化需要用随机梯度下降，而实际上随机梯度下降是为了满足在线学习的需要，如果是批量学习可以直接使用梯度学习算法实现。
  * 总结：瑕不掩瑜，建议结合其他书一起看。
* 笔记目的: 记录重点, 方便回忆。

# C01. 统计学习方法概论

* 这一章都是概念和结论，如果读者能够透过概念就明白里面实际操作的内容，那说明读者就可以不用再深入阅读了。
* 后面的各章内容相对独立，读者既可以连续学习，也可以仅选择自己感兴趣的内容。

## 1.1. 统计学习

### 统计学习导言

* 统计学习 (statistical learning)：关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。因此统计学习也称为统计机器学习 (statistical machine learning).
* 统计学习的主要特点：
  * 统计学习以计算机及网络为平台；
  * 统计学习以数据为研究对象，是数据驱动的学科；
  * 统计学习的目的是对数据进行预测和分析；
  * 统计学习以方法为中心，通过统计学习方法构建模型并应用模型进行预测和分析；
  * 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中形成独立的理论体系与方法论。

### 统计学习的对象

* 统计学习的对象是数据 (data)
  * 从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中。
  * 对数据的基本假设是同类数据具有一定的统计规律性。即某种共同性质的数据，因为具有的统计规律性，所以可以用概率统计方法加以处理。
  * 数据分类有 “连续型” 和 “离散型” 两种，本书主要关注的是 “离散型” 数据。

### 统计学习的目的：

* 学习什么样的模型
* 如何学习模型 → 使模型能够对数据进行准确地预测和分析
* 尽可能地提高模型的学习效率

### 统计学习的方法：

* 统计学习的方法分类：
  * **有监督学习 (supervised learning)**：（全书重点）
    * 从给定的、有限的、用于学习的训练数据 (training data) 集合出发，假设数据是独立同分布产生的；
    * 假设要学习的模型属于某个函数的集合，称为假设空间 (hypothesis space)；
    * 基于某个评价标准 (evaluation criterion)，从假设空间中选取一个最优的模型，使它在给定的评价准则下，对已知训练数据及未知测试数据 (test data) 都有最优的预测；
    * 最优模型的选取都由算法实现。
  * 无监督学习 (unsupervised learning):
  * 半监督学习 (semi-supervised learning):
  * 强化学习 (reinforcement learning):
* 统计学习方法的三个要素：
  * 模型 (model)：模型的假设空间；
  * 策略 (strategy)：模型选择的准则；
  * 算法 (algorithm)：模型学习的算法。
* 实现统计学习方法的步骤：
  * 得到一个有限用于训练的数据集合；
  * 确定包含所有可能的模型的假设空间，即学习模型的集合；
  * 确定模型选择的准则，即学习的策略；
  * 确定求解最优模型的算法，即学习的算法；
  * 通过学习方法选择最优模型；
  * 利用学习的最优模型对新数据进行预测或分析。
* 统计学习中的有监督学习根据 “解决的问题” 主要包括：
  * 分类问题
  * 标注问题
  * 回归问题

### 统计学习的研究：

* 统计学习方法 (statistical learning method)：开发新的学习方法；
* 统计学习理论 (statistical learning theory)：探求统计学习方法的有效性与效率，以及统计学习的基本理论问题；
* 统计学习应用 (application of statistical learning)：将统计学习方法应用到实际问题中去，解决实际问题。

### 统计学习的重要性：

* 是处理海量数据的有效方法；
* 是计算机智能化的有效手段；
* 是计算机科学发展的重要组成。

## 1.2. 监督学习

* 监督学习的任务：是学习一个模型，使模型能够对任意给定的输入，及其相应的输出做出一个好的预测

### 1.2.1. 基本概念

* 输入空间：输入数据所有可能取值的集合；集合中元素的个数可以有限，也可以是整个空间；
* 输出空间：输出数据所有可能取值的集合；集合中元素的个数可以有限，也可以是整个空间；
* 假设空间：由输入空间到输出空间的映射的集合，即可供选择的模型构成的空间；
* 特征空间：所有特征向量存在的空间。
  * 每个具体的输入是一个实例 (instance)，通常由特征向量 (feature vector) 表示。
* 统计学习中的有监督学习根据 “输入变量” 和 “输出变量” 的不同主要包括：
  * 分类问题：输出变量为有限个离散变量的预测问题；
  * 标注问题：输入变量与输出变量均为变量序列的预测问题；
  * 回归问题：输入变量与输出变量均为连续变量；
* 联合概率分布：输入变量与输出变量遵循联合分布；

### 1.2.2. 问题的形式化描述

* 在学习过程中，学习系统（也就是学习算法）试图通过给定的训练数据集中的样本带来的信息来学习得到模型。

## 1.3. 统计学习三个要素

* 统计学习方法 = 模型 + 策略 + 算法

### 1.3.1. 模型

* 主要问题：学习什么样的模型？
* 模型的假设空间：包含所有可能的条件概率分布或决策函数，即由一个参数向量决定的函数族，也称为参数空间 (parameter space)。
* 模型分类：
  * 非概率模型：由决策函数表示的模型；
  * 概率模型：由条件概率表示的模型；

### 1.3.2. 策略

* 主要问题：按照什么样的准则，学习得到最优的模型，或者从假设空间中选择最优的模型。
* 基本概念：
  * 损失函数 (loss function) 或代价函数 (cost function)：度量模型一次预测的好坏；
  * 风险函数 (risk function) 或期望损失 (expected loss)：度量平均意义下模型预测的好坏。
  * 经验风险 (empirical risk) 或经验损失 (empirical loss)：关于模型训练样本集的平均损失，当样本容量趋于无穷时，经验风险逼近期望风险；
  * 结构风险 (structural risk)：表示模型复杂度的正则化项 (regularizer) 或惩罚项 (penalty term)。
* 常用的损失函数：
  * 0-1 损失函数
  * 平方损失函数
  * 绝对值损失函数
  * 对数损失函数或对数似然损失函数
* 学习目标：
  * 理想状态：就是选择期望风险或期望损失最小的模型，希望数据是无限的；
  * 现实状态：就是选择经验风险或经验损失最小的模型，因为数据是有限的；
* 经验风险矫正：当样本容量过小时，容易出现 “过拟合” 问题，所以需要对经验风险进行矫正，经验风险最小化 + 结构风险最小化
  * 经验风险最小化 (empirical risk minimization, ERM)：极大似然估计
  * 结构风险最小化 (structural risk minimization, SRM)：最大后验估计

### 1.3.3. 算法

* 统计学习是基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。
* **算法** 即计算方法。统计学习的算法就转化为求解最优化问题的算法。
  * 有显式的解析解的最优化问题；
  * 无显式的解析解的最优化问题，需要用数值计算的方法求解。
    * 如何保证找到全局最优解；
    * 如何保证求解的过程高效。

## 1.4. 模型的评估与选择

* 1.4~1.7，与模型选择有关的问题。
* 1.8~1.10，与模型应用有关的问题。

### 1.4.1. 模型评估

* 学习方法评估的标准
  * 基于损失函数的模型的训练误差 (training error)：用来评估一个学习问题是否容易学习
  * 基于损失函数的模型的测试误差 (test error)：用来评估一个模型是否具备更有效的预测
* 泛化能力 (generalization ability)：学习方法对未知数据的预测能力

### 1.4.2. 模型选择

* 过拟合 (over-fitting)：学习时选择的模型所包含的参数过多，以至于模型对已知数据预测较好，未知数据预测较差的问题
* 模型选择的常用方法：
  * 正则化
  * 交叉验证

## 1.5. 正则化与交叉验证

### 1.5.1. 正则化

* 正则化 (regularization)：结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。
  * 正则化项一般是模型复杂度的单调递增函数。
    * 复杂度定义可以参考 Kolmogorov 复杂性理论 (complexity theory) \[Haykin，2011] P48
  * Occam 剃刀原理：应用于模型选择时符合正则化的想法，即所有能够解释数据的模型中，复杂度越小越好。
  * Bayes 估计：正则化项对应于模型的先验概率。数据较少时先验概率就可以抑制数据中噪声的干扰，防止出现过拟合问题。数据很多时，先验概率就让位于数据对模型的解释。

### 1.5.2. 交叉验证

* 交叉验证 (cross validation)：在数据充足时，随机地将数据切分成三个部分：训练集、验证集和测试集。选择对验证集有最小预测误差的模型。
  * 训练集 (training set)：用来训练模型；
  * 验证集 (validation set)：用来选择模型；
  * 测试集 (test set)：用来评估模型。
* 交叉验证的常用方法：
  * 简单交叉验证：随机地将数据分成两个部分，70% 的数据为训练集，30% 的数据为测试集，选择测试误差最小的模型；
  * S 折交叉验证：
    * 随机地将数据分成 S 个互不相交的大小相同的部分
    * 然后利用 S-1 个部分的数据训练，1 个子集测试模型，
    * 再将这一个过程对所有可能的选择重复进行，
    * 最后选择 S 次评测中平均测试误差最小的模型。
  * 留一交叉验证：当 S=N 时采用的 S 折交叉验证，适用于数据极度缺乏的情况下。（N 为给定数据集的容量）

## 1.6. 泛化能力

### 1.6.1. 泛化误差

* 泛化能力 (generalization ability)：是指学习方法学习到的模型对未知数据的预测能力
* 泛化误差 (generalization error)：是批学到的模型对未知数据预测产生的误差，反映了学习方法的泛化能力。

### 1.6.2. 泛化误差的上界

* 泛化误差的上界 (generalization error bound)：泛化误差的概率上界，通过比较两种学习方法的泛化误差的上界来确定优劣
* 泛化误差上界的性质：
  * 是样本容量的函数，当样本容量增加时，泛化上界趋向于 0；
  * 是假设空间的函数，当假设空间容量增加时，泛化误差上界就会变大，表示模型就更加难学。
* _泛化误差上界定理及证明_（建议放弃）

## 1.7. 生成模型与判别模型

* 生成模型 (generative model)：模型表示了给定输入 X 产生输出 Y 的生成关系。
  * 特点：
    * 还原出联合概率分布；
    * 学习收敛速度快；
    * 样本容量增加时，能够更好地逼近真实模型；
    * 存在隐变量时，仍然可以使用。
  * 应用：朴素 Bayes 方法和隐马尔可夫模型 (Hidden Markov Model, HMM)；
  * 注：生成模型是比较难理解的概念，HMM 是理解生成模型比较好的途径，如果对HMM感兴趣可以参考：
    * 简单了解： \[周志华, 2018] P320
    * 深入理解： \[Rabiner, 1989]
* 判别模型 (discriminative model)：由数据直接学习决策函数或者条件概率分布作为预测的模型
  * 特点：
    * 直接学习得到条件概率分布或者决策函数；
    * 直接面对预测，学习的准确率更高；
    * 基于参数是直接学习得到的，因此可以对数据进行各种程度上的抽象、定义和使用特征，简化学习问题。
  * 应用：k 近邻法、感知机、决策树、Logistic 回归模型、最大熵模型、支持向量机、提升方法和条件随机场等

## 1.8. 分类问题

* 分类器 (classifier)：监督学习从数据中学习得到的分类模型或分类决策函数。
* 分类 (classification）：利用分类器对新的输入进行输出的预测。
* 解决分类问题的两个过程：
  * 学习过程：根据已知的训练数据集利用有效的学习方法学习得到一个分类器；
  * 分类过程：利用学习得到的分类器对新的输入实例进行分类。
* 评价分类器性能的指标：分类准确率 (accuracy)，即对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。
  * 二类分类问题常用的评价指标：精确率 (precision) 与召回率(recall)。
* 解决分类问题的常用方法：k 近邻法、感知机、朴素 Bayes 法，决策树、决策列表、Logistc 回归模型、支持向量机、提升方法等

## 1.9. 标注问题

* 标注问题：是分类问题的推广，也是更复杂的结构预测问题的简单形式。
  * 输入是一个观测序列；
  * 输出是一个标记序列或状态序列。
  * 目标是通过学习得到能够对观测序列给出标记序列作为预测的模型。
* 解决标注问题的两个过程：学习过程 和 标注过程
* 评价标注问题的指标：准确率、精确率和召回率。
* 解决标注问题的常用方法：隐 Markov 模型和条件随机场。

## 1.10. 回归问题

* 回归 (regression)：用于预测输入变量（自变量）和输出变量（因变量）之间的关系。
* 回归模型：表示从输入变量到输出变量之间的映射关系的函数。
  * 等价于：函数拟合。
* 解决回归问题的两个过程：学习过程和预测过程。
* 回归问题的分类
  * 按输入变量的个数：一元回归和多元回归；
  * 按输入变量和输出变量之间的关系：线性回归和非线性回归。
* 回归学习最常用的损失函数：平方损失函数，求解平方损失函数可以用最小二乘法。

# C02. 感知机

* 模型：
  * 感知机，是根据输入实例的特征向量对其进行二类分类的线性分类模型，属于判别模型；
  * 模型参数包括：权值或权值向量，偏置。
  * 模型对应于输入空间（特征空间）中的分离超平面；
* 策略：
  * 假设：感知机学习的训练数据集是线性可分的；
  * 目标：求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面；
  * 策略：即定义（经验）损失函数，并将损失函数极小化；
    * 损失函数定义为：误分类点的总数，不易优化；
    * 损失函数定义为：误分类到分离超平面的总距离；
* 算法：
  * 感知机学习算法是基于误差 - 修正的学习思想，是由误分类驱动的；
  * 学习算法的优化方法：
    * 批量学习可以基于多种方式优化：
      * 一阶：最速下降法或梯度下降法；
      * 二阶：牛顿法、共轭梯度法等等
    * 在线学习：基于随机梯度下降法的对损失函数进行最优化 \[Goodfellow, 2017] P95, P180
      * 原始形式：算法简单且易于实现。先任意选取一个超平面，然后随机选择一个误分类点使其用梯度下降法极小化目标函数
        * _例 2.1_（比较简单，可以了解）
        * _定理 2.1_（过于简略，可以跳过）
      * _对偶形式：_ （没看出与原始形式有何区别，也没从别的书上看到过这种说明方式，建议跳过）
  * 当训练数据集线性可分时，感知机学习算法是收敛的，且有无穷多个解。
* **学习总结**
  * 感知机是神经网络的基础，本章只有单个神经元模型，详细学习参考 \[Haykin，2011]
  * 神经网络是深度学习的基础，需要了解深度学习参考 \[Goodfellow, 2017]
  * 距离度量是来自于数学中几何的概念，只想了解基础可参考 \[Duda, 2003] P154
  * 学习算法的优化是来自于最优化理论的概念，只想了解基本优化方法可参考 \[Hyvarinen, 2007] P42

# C03.  **k** _近邻法_

* **k** 近邻法 (k-nearest neighbor, k-NN) 是基本且简单的分类与回归方法。
  * 输入为实例的特征向量，对应于特征空间的点；
  * 输出为实例的类别，可以取多个类。
* 基本思想：
  * 假设给定一个训练数据集，其中的实例类别已经确定；
  * 对新的实例分类时，根据其 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。
  * 不具有显式的学习过程。
  * 实际上利用训练数据集对特征向量空间进行切分，并作为其分类的 “模型”。
* **k** 近邻的模型：
  * 对应于基于训练数据集对特征空间的一个划分。
  * 当训练集、距离度量、k 值及分类决策规则确定后，输入实例所属类别也唯一确定。
* **k** 近邻法的三个要素：
  * 距离度量：常用欧氏距离；（距离定义）\[Duda, 2003]
  * k 值的选择：反映了近似误差与估计误差之间的权衡。
    * k 值越大时，近似误差会增大，估计误差会减小，模型也越简单；
    * k 值越小时，近似误差会减少，估计误差会增大，模型也越复杂。
    * 可以用交叉验证的方式选择最优 k 值。
  * 分类决策规则：多数表决规则 (marjority voting rule)，等价于 经验风险最小化。
* **k** _近邻法的实现基于 kd 树_。（了解即可，实际应用多是成熟的软件包）
  * kd 树是一种便于对 k 维空间中的数据进行快速检索的数据结构；
  * kd 树是二叉树，表示对 k 维空间的一个划分；
  * kd 树的每个圣战对应于 k 维空间划分中的一个超矩形区域；
  * 利用 kd 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。
* **学习总结**
  * 了解即可，因为面对高维问题效果很差，需要考虑降维操作。\[周志华, 2018] P225

# C04. 朴素 Bayes 法

* 朴素 (naive) Bayes 法：是基于 Bayes 定理与所有特征都遵循条件独立性假设的分类方法。
  * 朴素 Bayes 法是 Bayes 分类法的一种，遵循 Bayes 定理建模。\[Mitchell, 2003] P112
  * 朴素 Bayes 法基于的条件独立性假设是说用于分类的特征在类别确定的条件下都是条件独立的。简化了计算复杂度，牺牲了分类准确率。
  * 朴素 Bayes 法是生成学习方法。
    * 先验概率分布；
    * 条件概率分布；
    * 后验概率分布。后验概率最大化准则等价于期望风险最小化准则。
    * 目标：由训练数据学习联合概率分布；
  * 朴素 Bayes 方法的概率参数估计方法：
    * **极大似然估计** ：大部分概率估计常用的方法；
    * **Bayes 估计** ：重点在于了解与极大似然估计的差别，才可以在正确的地方使用。
* **学习总结**
  * 虽然不需要自己估计参数，但是对估计的理解很重要，书中写的过于简单，具体内容请参考 \[Duda, 2003] P67

# C05. 决策树 ( decision tree )

* 决策树模型：
  * 决策树是一种基本的分类与回归方法。
    * 本章主要讨论的是分类决策树。
  * 分类决策树模型：
    * 定义：是基于特征对实例进行分类的树形结构。
    * 模型的组成结构：
      * 结点(node)：
        * 内部结点(internal node)
        * 叶结点(leaf node)
      * 有向边(directed edge)
    * 分类决策树可以转换成一个if-then规则的集合；
      * 决策树的根结点到叶结点的每一条路径构建一条规则；
      * 路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。
      * 重要的性质：互斥并且完备，即全覆盖。
        * 覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。
    * 也可以看作是定义在特征空间与类空间上的条件概率分布。
      * 这个条件概率分布定义在特征空间的一个划分上，
      * 将特征空间划分为互不相交的单元或区域，
      * 并在每个单元定义一个类的概率分布就构成了一个条件概率分布。
      * 决策树分类时，将结点的实例分到条件概率大的类中。
    * 主要优点：可读性强，分类速度快。
* 决策树学习
  * 学习目的
    * 根据给定的训练数据集，构建一个与训练数据拟合很好，并且复杂度小的决策树，使之能够对实例进行正确的分类。
    * 决策树与训练数据的矛盾较小，同时还具有较好的泛化能力。
    * 也可以看作由训练数据集估计条件概率模型
      * 模型对训练数据有很好的拟合；
      * 模型对未知数据有很好的预测。
    * 从所有可能的决策树中选取最优决策树是NP完全问题；
      * 现实中采用启发式方法学习次优的决策树。
  * 学习原则：损失函数最小化。
    * 损失函数是一种正则化的极大似然函数
  * 决策树的学习算法
    * 递归地选择最优特征，并根据该特征对训练数据进行分割，使之对各个数据集有一个最好的分类的过程。
    * 学习算法包括3个部分：
      * 特征选择：
        * 特征选择的目的在于选取对训练数据能够分类的特征，提高决策树学习的效率；
        * 特征选择的关键是其准则：
          * 样本集合D对特征A的 **信息增益** 最大
            * 信息增益定义为集合D的经验熵与特征A在给定条件下D的经验条件熵之差。
              * 熵：表示随机变量不确定性的度量。也称为经验熵。
              * 条件熵：定义为X给定条件下Y的条件概率分布的熵对X的数学期望。也称为经验条件熵。
            * 信息增益表示得知特征 X 的信息而使得类 Y 的信息的不确定性减少的程度。
            * 信息增益等价于训练数据集中类与特征的互信息。
            * 信息增益依赖于特征，信息增益大的特征具有更强的分类能力。
          * 样本集合D对特征A的 **信息增益比** 最大
            * 为了避免信息增益对取值较多的特征的偏重，使用信息增益比来代替；
            * 信息增益比：特征A对训练数据集D的信息增益与训练数据集D关于特征A的值的熵之比。
          * 样本集合D的 **基尼指数** 最小
      * 树的生成：
        * 计算指标，再根据准则选取最优切分点，从根结点开发，递归地产生决策树。
        * 通过不断地选择局部最优的特征，得到可能是全局次优的结果。
      * 树的剪枝：将已经生成的树进行简化的过程。
        * 目的：由于生成的决策树存在过拟合问题，需要对它进行剪枝，以简化学到的决策树。
        * 剪枝的准则：极小化决策树整体的损失函数或代价函数，等价于正则化的极大似然估计。
        * 剪枝的分类
          * 预剪枝：也叫分支停止准则。在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；
          * 后剪枝：先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。
  * 常用的学习算法：
    * ID3：在决策树的各个结点上应用信息增益准则选择特征，递归地构建决策树。相当于用极大似然法进行概率模型的选择。
    * C4.5：在决策树的各个结点上应用信息增益比准则选择特征，递归地构建决策树。
    * CART：既可用于分类，也可用于回归。
      * 等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。
      * CART算法的两个过程：
        * 决策树生成：基于训练数据集生成决策树，要尽量大；
          * 回归树生成：
            * 用平方误差最小的准则求解每个单元上的最优输出值。
            * 回归树通常称为最小二乘回归树。
          * 分类树生成：
            * 用基尼指数选择最优特征，并决定该特征的最优二值切分点。
            * 算法停止计算的条件
              * 结点中的样本个数小于预定阈值；
              * 样本集的基尼小于预定阈值；
        * 决策树剪枝：
          * 用验证数据集对已经生成的树进行剪枝，剪枝的标准为损失函数最小，基于标准选择最优子树。
          * 可以通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。        * 
        * \[Duda, 2003] P320，CART作为通用的框架，定义了6个问题
* 决策树的预测：
  * 对新的数据，利用决策树模型进行分类。
* **学习总结**
  * 算法 ( 5.1, 5.2，5.6 ) + 例题 ( 5.1, 5.2, 5.3, 5.4 )：通过算法和例题可以增强理解；
  * 损失函数的定义可以进一步参考“不纯度”指标 \[Duda, 2003] P320，或“纯度”指标 \[周志华, 2018] P75
    * “不纯度”指标是求极小值，可以跟梯度下降法等最优化理论结合。

# 参考文献

* \[Duda, 2003] Duda R O, PeterEHart D G S, 李宏东等庋. 模式分类 \[M]. 机械工业出版社. 2003.
* \[Goodfellow, 2017] Goodfellow I, Bengio Y, Courville A. 深度学习 \[M]. 人民邮电出版社. 2017.
* \[Haykin，2011] Haykin S . 神经网络与机器学习 \[M]. 机械工业出版社. 2011.
* \[Hyvarinen, 2007] Aapo Hyvarinen, Juha Karhunen. 周宗潭译 独立成分分析 \[M]. 电子工业出版社. 2007.
* \[Mitchell, 2003] Tom M.Mitchell. 肖华军等译. 机器学习 \[M]. 机械工业出版社. 2003
* \[Rabiner, 1989] Rabiner L R. A tutorial on hidden Markov models and selected applications in speech recognition[J]. Proceedings of the IEEE, 1989, 77(2): 257-286.
* \[周志华, 2018] 周志华 机器学习 \[M]. 清华大学出版社. 2018