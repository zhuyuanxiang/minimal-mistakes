---
title: "2020 年 06 月日记"
excerpt: "按周分类，每天工作和心得记录。"
categories:
-   日记
tags:
-   工作
-   学习
-   生活
-   感想
create_at: 2020-06-01
last_modified_at: 2020-06-30
toc: true
toc_label: "文章提纲"
toc_icon: "book-reader"
toc_sticky: true
---

# Week List

## the First week

-   2020-06-01
    -   参加腾迅广告大赛
        -   与团队成员沟通，确认数据处理方式
-   2020-06-02
    -   参加腾迅广告大赛
        -   准备数据
        -   了解 MySQL 的引擎
        -   撰写数据处理文档
-   2020-06-03
    -   参加腾迅广告大赛
        -   整理数据
            1.  统计数据的视图合并
            2.  创建每个 creative_id 的统计数据
            3.  创建每个 user_id 的统计数据
            4.  创建 tf-idf 计算表，全部数据都保留在结果中，排序在 train_creative_id 中完成，保留素材的个数在程序的代码中完成
            5.  创建素材词典，重新编码 creative_id
            6.  创建用户词典，重新编码 user_id
            7.  创建训练数据，沿用 `all_log_valid_1m` 中的数据，重新编码 `creative_id` 和 `user_id`
    -   解决大型数据集遇到的 Memory Error 问题
        -   使用稀疏矩阵
-   2020-06-04
    -   参加腾迅广告大赛
        -   解决 Memory Error 问题：使用 64 位的 Python + 扩大 Windows 的虚拟内存
        -   创建训练 Word2Vec 的数据
        -   使用 嵌入层 训练网络：效果不好
        -   使用 One-Hot 编码后的数据训练，精确度有所提高
            -   因为数据的稀疏性问题，使用 L1 正则有所帮助，减少素材库和用户库的大小也有所帮助
-   2020-06-05
    -   参加腾迅广告大赛
        -   1m 的数据准备，撰写 Word2Vec 算法代码，使用 Keras 的 Embedding 层和 MLP 学习 Age 分类，精确度最高可以达到 87%，数据量不够大，算法容易过拟合，数据提供的信息不足
        -   确定 user_id 的数目 ( 决定了训练数据集的大小 )
        -   确定 batch_size 与 user_id 的关系
        -   确定 creative_id 的数目与之的关系 ( 决定了词典的大小 )
        -   确定 epoches 与 creative_id 的关系
        -   确定 embedding_size 与 creative_id 的关系
-   2020-06-06
    -   参加腾迅广告大赛
        -   将 1m 的数据中没有访问素材的天用 0 填充，增加 1 作为每个用户数据开始序列
        -   增加实验日志文件，记录实验效果

## the Second week

-   2020-06-07
    -   参加腾迅广告大赛
        -   重新基于「用户访问素材」的比例来提取数据，因为原来基于「product_id」的提取方式不正确
        -   数据预处理模块 preprocessing.py
        -   网络构建模型 network.py
        -   重构 MLP-W2V-Gender-Keras.py
        -   总结
            -   网络结构最优的就是 MLP，但是参数很难调试，对于大数据量基本不可能使用
            -   使用 MLP 作为基准，测试其他方便快捷的学习方式
            -   稠密向量的维度可以低一些，可能跟数据本身分类有限有关系
            -   网络很容易过拟合，注意降低网络复杂度，还可能跟数据量太少有关系
-   2020-06-08
    -   参加腾迅广告大赛
        -   购买新的电脑，必要的投入是需要的
        -   登记 50 万数据表的统计数据
        -   500K 数据表的统计信息
            -   创建 tf-idf 数据
            -   创建训练数据
        -   更新 50万 和 500万的统计数据
        -   重构：5m_51 的统计数据表
        -   重构：数据预处理模块
        -   重构：网络构建模块
        -   提取数据处理模块和网络构建模块的函数
        -   删除错误数据日志，重新构建不同网络结构计算的结果
-   2020-06-09
    -   参加腾迅广告大赛
        -   安装新的电脑环境：CUDA+cuDNN+Tensorflow-GPU+PyCharm+Navicat+MySQL+Pyton+AnaConda+Git+GitHub+HuoRong+QQ Wubi+VSCode+Office 2013+
-   2020-06-10
    -   参加腾迅广告大赛
        -   准备 3m 的 Word2Vec 训练数据，本地训练成功后，上传到服务器进行训练
        -   使用 Conv1D 训练网络
        -   使用 RNN 或者 LSTM 训练网络
        -   创建全部字段的全部数据的表
        -   格式修正
        -   重构：数据预处理模块
        -   Conv1D实验报告
        -   基于 深度学习 训练模型
-   2020-06-11
    -   参加腾迅广告大赛
        -   编写测试 Gender 问题的分类代码
        -   构建所有数据的统计信息
        -   修正代码错误
            -   增加异常控制代码
        -   简化模型构建代码
        -   原始数据统计信息
        -   调用统一的「网络构建函数」来训练
        -   重构「网络模型构建」函数，将各种函数统一为参数配置
        -   补充输出实验报告需要的数据
        -   对「age」字段进行学习
        -   对「age」进行网络训练
            -   注：记得修改模型
        -   增加
            -   construct_GlobalMaxPooling1D()：构造全局最大池化模型
        -   增加
            -   load_data()：载入数据函数
            -   data_sequence_no_start()：生成数据序列，不包含文章开始标志
        -   重新输出完整的数据集
            -   有 time_id 的时间序列数据
            -   无 time_id 的非时间序列数据
-   2020-06-12
    -   参加腾迅广告大赛
    -   计划
        -   安装 Typora
-   2020-06-13
    -   参加腾迅广告大赛

## the Third week

-   2020-06-14
    -   参加腾迅广告大赛
-   2020-06-15
    -   参加腾迅广告大赛
-   2020-06-16
    -   参加腾迅广告大赛
-   2020-06-17
    -   参加腾迅广告大赛
-   2020-06-18
    -   参加腾迅广告大赛
-   2020-06-19
    -   参加腾迅广告大赛
-   2020-06-20
    -   参加腾迅广告大赛

## the Fourth week

-   2020-06-21
    -   参加腾迅广告大赛
-   2020-06-22
    -   参加腾迅广告大赛
-   2020-06-23
-   2020-06-24
-   2020-06-25
-   2020-06-26
-   2020-06-27

## the Fifth week

-   2020-06-28
-   2020-06-29
-   2020-06-30
-   2020-06-31
